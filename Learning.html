<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="favicon.ico" />
<link rel="bookmark" href="favicon.ico" type="image/x-icon"　/>
<title>Yizhuo Zheng (郑怡卓)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="Learning.html" class="current">Learning</a></div>
<div class="menu-item"><a href="https://github.com/amark071">GitHub</a></div>
<div class="menu-item"><a href="https://blog.csdn.net/hhhjjjjio?type=blog">CSDN</a></div>
</td>
<td id="layout-content">

  <div class="sidebar" style="position: fixed; top: 20px; right: 30px; width: 500px;">
    <h3>Page Navigation</h3>
    <ul>
      <li><a href="#partm1">Part M.I: Mathematical Statistics</a></li>
      <li><a href="#partm2">Part M.II: Mathematical Introduction to Machine Learning</a></li>
      <li><a href="#partm3">Part M.III: Numerical Algebra & Analysis</a></li>
      <li><a href="#partm4">Part M.IV: Optimization Algorithms</a></li>
      <li><a href="#partc1">Part C.I: Data structures and algorithms</a></li>
      <li><a href="#partc2">Part C.II: Parallel and Distributed Computing</a></li>

    </ul>
  </div>  


<div id="toptitle">
<h1>Learning of Computer Science and Mathmatic</h1>
</div>
<h2>Introduction to the page</h2>
<p>This page is used to note what I have learned in university and when working.</p>

<h2>What I have learned in math</h2>

<h3 id="partm1">Part I : Mathematical Statistics</h3>
<ol>

<li><p>Character 2 : <strong>Estimation(Including the Introduction)</strong> <a href="math/I/character2.pdf">[note]</a>
  <ul> 
    <li><a href="math/I/character2.pdf#page=13">(P13)</a>Methods of parameter estimation (MLE & ME)  </li> 
    <li><a href="math/I/character2.pdf#page=30">(P30)</a>Estimated goodness criteria  </li> 
    <li><a href="math/I/character2.pdf#page=53">(P53)</a>Confidence interval  </li> 
    <li><a href="math/I/character2.pdf#page=76">(P76)</a>Estimation of distribution function and density function </li> 
  </ul>
</p></li>

<li><p>Character 3 : <strong>Hypothesis Testing</strong> <a href="math/I/character3.pdf">[note]</a>
  <ul> 
    <li><a href="math/I/character3.pdf#page=1">(P1)</a>Formulation of the question</li> 
    <li><a href="math/I/character3.pdf#page=9">(P9)</a>N-P lemma and likelihood ratio test</li> 
    <li><a href="math/I/character3.pdf#page=19">(P19)</a>Hypothesis testing for single-parameter cases</li>
    <li><a href="math/I/character3.pdf#page=38">(P38)</a>Generalized likelihood ratio test</li>
    <li><a href="math/I/character3.pdf#page=69">(P69)</a> Cut-off and p-value</li>
    <li><a href="math/I/character3.pdf#page=71">(P71)</a> Goodness-of-fit test</li>
  </ul>
</p></li>

<li><p>Character 4 : <strong>Regression Analysis and Linear Models</strong> <a href="math/I/character4.pdf">[note]</a>
  <ul> 
    <li><a href="math/I/character4.pdf#page=1">(P1)</a> Introduction</li> 
    <li><a href="math/I/character4.pdf#page=5">(P5)</a> Univariate linear regression</li> 
    <li><a href="math/I/character4.pdf#page=22">(p22)</a> Parameter estimation for linear models</li> 
    <li><a href="math/I/character4.pdf#page=52">(P52)</a> Hypothesis testing for linear models</li>
    <li><a href="math/I/character4.pdf#page=68">(P68)</a> Regression analysis</li>
  </ul>
</p></li>

<li><p>Character 5 : <strong> Design of Experiments and Analysis of Variance</strong> <a href="math/I/character5.pdf">[note]</a>
  <ul> 
    <li><a href="math/I/character5.pdf#page=1">(P1)</a>ANOVA for full trials</li> 
    <li><a href="math/I/character5.pdf#page=18">(P18)</a>Orthogonal design</li>
  </ul>
</p></li>

<li><p>Character 6 : <strong>Sequential Analysis</strong> <a href="math/I/character6.pdf">[note]</a>
  <ul> 
    <li><a href="math/I/character6.pdf#page=1">(P1)</a>The importance of the sequential approach and two elements</li> 
    <li><a href="math/I/character6.pdf#page=6">(P6)</a>TSequential probability ratio test</li> 

  </ul>
</p></li>

<li><p>Character 7 : <strong>Statistical Decision-making and Bayesian Statistical Generalis</strong> <a href="math/I/character78.pdf">[note]</a>
  <ul> 
    <li><a href="math/I/character78.pdf#page=8">(P8)</a>Overview of statistical decision-making issues</li> 
    <li><a href="math/I/character78.pdf#page=17">(P17)</a>Bayesian statistics</li> 
    <li><a href="math/I/character78.pdf#page=35">(P35)</a>A priori distribution</li> 
    <li><a href="math/I/character78.pdf#page=51">(P51)</a>Stochastic simulation of the martensitian chain</li> 
    <li><a href="math/I/character78.pdf#page=65">(P65)</a> Overview of sample surveys</li> 

  </ul>
</p></li>
</ol>

<h3 id="partm2">Part II : Mathematical Introduction to Machine Learning</h3>
<ol>

<li><p>Character 1 : <strong>Introduction</strong> <a href="math/II/lecture-1.pdf">[note]</a></p></li>
<li><p>Character 2 : <strong>Linear method for regression</strong> <a href="math/II/lecture-2.pdf">[note]</a></p></li>
<li><p>Character 3 : <strong>Classification</strong> <a href="math/II/lecture-3.pdf">[note]</a></p></li>
<li><p>Character 4 : <strong>Unsupervised learning</strong> <a href="math/II/lecture-4.pdf">[note]</a></p></li>
<li><p>Character 5 : <strong>GD and momentum accelerations</strong> <a href="math/II/lecture-5.pdf">[note]</a></p></li>
<li><p>Character 6 : <strong>SGD</strong> <a href="math/II/lecture-6.pdf">[note]</a></p></li>
<li><p>Character 7 : <strong>Introduction to Neural Network</strong> <a href="math/II/lecture-7.pdf">[note]</a></p></li>
<li><p>Character 8 : <strong>Training of Neural Network</strong> <a href="math/II/lecture-8.pdf">[note]</a></p></li>
<li><p>Character 9 : <strong>Introduction to Pytorch</strong> <a href="math/II/lecture-9.pdf">[note]</a></p></li>
<li><p>Character 10 : <strong>High-dimensional Distribution Learning</strong> <a href="math/II/lecture-10.pdf">[note]</a></p></li>
<li><p>Character 11 : <strong>Diffusion models and score matching</strong> <a href="math/II/lecture-11.pdf">[note]</a></p></li>
<li><p>Character 12 : <strong>Transformer and LLM</strong> <a href="math/II/lecture-12.pdf">[note]</a></p></li>
<li><p>Character 13 : <strong>Concentration inequalities</strong> <a href="math/II/lecture-13.pdf">[note]</a></p></li>
<li><p>Character 14 : <strong>Uniform concentration and generalization bounds</strong> <a href="math/II/lecture-14.pdf">[note]</a></p></li>
<li><p>Character 15 : <strong>Theorical foundation of kernel methods</strong> <a href="math/II/lecture-15.pdf">[note]</a></p></li>
<li><p>Character 16 : <strong>Theorical foundation of 2-layer Neural Networks</strong> <a href="math/II/lecture-16.pdf">[note]</a></p></li>

</ol>

<h3 id="partm3">Part III : Numerical Algebra & Analysis</h3>
<ol>

<li><p>Character 1 : <strong>Direct Solution of Linear Equations</strong> <a href="math/III/01.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/01.pdf#page=4">(P4)</a>The solution of a system of trigonometric equations  </li> 
    <li><a href="math/III/01.pdf#page=9">(P9)</a>Trigonometric decomposition and  Elect the principal triangular decomposition</li> 
    <li><a href="math/III/01.pdf#page=34">(P34)</a>Cholesky decomposition method  </li>
    <li><a href="math/III/01.pdf#page=45">(P45)</a>Banded Gaussian elimination method  </li>
    <li><a href="math/III/01.pdf#page=50">(P50)</a>Block triangulation decomposition  </li>
  </ul>
</p></li>

<li><p>Character 2 : <strong>Round-off Error Analysis</strong> <a href="math/III/02.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/02.pdf#page=4">(P4)</a>Rounding error analysis for basic operations  </li> 
    <li><a href="math/III/02.pdf#page=17">(P17)</a>Rounding error analysis for Gaussian elimination method  </li> 
    <li><a href="math/III/02.pdf#page=30">(P30)</a>Accuracy estimation and iterative improvement of computational solutions  </li> 
  </ul>
</p></li>

<li><p>Character 3 : <strong>Norm and Sensitivity Analysis</strong> <a href="math/III/03.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/03.pdf#page=4">(P4)</a>Vector norm  </li> 
    <li><a href="math/III/03.pdf#page=10">(P10)</a>Matrix norm  </li> 
    <li><a href="math/III/03.pdf#page=25">(P25)</a>Sensitivity analysis  </li> 
  </ul>
</p></li>

<li><p>Character 4 : <strong>Iterative Method to Solve Linear Equations</strong> <a href="math/III/04.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/04.pdf#page=4">(P4)</a>Jacobi iteration and Gauss-Seidel iteration  </li> 
    <li><a href="math/III/04.pdf#page=8">(P8)</a>Convergence of the iterative method  </li> 
    <li><a href="math/III/04.pdf#page=52">(P52)</a>Ultra-slack iterations  </li> 
  </ul>
</p></li>

<li><p>Character 5 : <strong>Fastest Descent Method and Conjugate Gradient Method</strong> <a href="math/III/05.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/05.pdf#page=4">(P4)</a>Fastest Descent Method  </li> 
    <li><a href="math/III/05.pdf#page=15">(P15)</a>Conjugate Gradient Method</li> 
    <li><a href="math/III/05.pdf#page=29">(P29)</a>Other methods of conjugate gradient method  </li> 
    <li><a href="math/III/05.pdf#page=42">(P42)</a>Generalized minimal remainder method  </li> 
  </ul>
</p></li>

<li><p>Character 6 : <strong>V-Cycle Method(Multiple meshes Method)</strong> <a href="math/III/06.pdf">[note]</a>
</p></li>

<li><p>Character 7 : <strong>Methods for calculating asymmetric eigenvalue problems</strong> <a href="math/III/07.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/07.pdf#page=4">(P4)</a>Introduction  </li> 
    <li><a href="math/III/07.pdf#page=9">(P9)</a>Power method & anti-power method & QR method & Subspace iterative method</li> 
    <li><a href="math/III/07.pdf#page=33">(P33)</a>Variation of Hessenberg  </li> 
    <li><a href="math/III/07.pdf#page=47">(P47)</a>QR iteration with displacement  </li> 
  </ul>
</p></li>

<li><p>Character 8 : <strong>Methods for calculating symmetric eigenvalue problems</strong> <a href="math/III/08.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/08.pdf#page=4">(P4)</a>Important lemmas  </li> 
    <li><a href="math/III/08.pdf#page=13">(P13)</a>Symmetrical QR method & Jacobi method & dichotomy & The law of divide and conquer</li>
    <li><a href="math/III/08.pdf#page=70">(P70)</a>Calculation of singular value decomposition  </li> 
  </ul>
</p></li>

<li><p>Character 9 : <strong>Polynomial Interpolation</strong> <a href="math/III/09.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/09.pdf#page=10">(P10)</a>Polynomial interpolation  </li> 
    <li><a href="math/III/09.pdf#page=40">(P40)</a>Spline interpolation</li>
    <li><a href="math/III/09.pdf#page=53">(P53)</a>Catch-up method </li> 
  </ul>
</p></li>

<li><p>Character 10 : <strong>Best Approximation</strong> <a href="math/III/10.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/10.pdf#page=4">(P4)</a>Normalize linear space  </li> 
    <li><a href="math/III/10.pdf#page=15">(P15)</a>Optimal square approximation</li>
    <li><a href="math/III/10.pdf#page=36">(P36)</a>Best consistent approximation </li> 
  </ul>
</p></li>

<li><p>Character 11 : <strong>Numerical Integration</strong> <a href="math/III/11.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/11.pdf#page=4">(P4)</a>The basic formula for numerical integration  </li> 
    <li><a href="math/III/11.pdf#page=10">(P10)</a>Compound integration formula</li>
    <li><a href="math/III/11.pdf#page=20">(P20)</a>Gauss integration formula </li> 
    <li><a href="math/III/11.pdf#page=26">(P26)</a>Accelerated convergence technology(Romberg) </li> 
  </ul>
</p></li>

<li><p>Character 12 : <strong>Numerical Solution of nonlinear equations</strong> <a href="math/III/12.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/12.pdf#page=7">(P7)</a>Iterative solution of nonlinear equations  </li> 
    <li><a href="math/III/12.pdf#page=36">(P36)</a>Iterative solution of nonlinear systems of equations  </li> 
  </ul>
</p></li>

<li><p>Character 13 : <strong>Numerical Solution of ODE</strong> <a href="math/III/13.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/13.pdf#page=4">(P4)</a>Euler method  </li> 
    <li><a href="math/III/13.pdf#page=14">(P14)</a>Runge-Kutta method  </li> 
    <li><a href="math/III/13.pdf#page=24">(P24)</a>Convergence and stability of the single-step method  </li> 
  </ul>
</p></li>

<li><p>Character 14 : <strong>Numerical Solution of PDE</strong> <a href="math/III/14.pdf">[note]</a>
  <ul> 
    <li><a href="math/III/14.pdf#page=4">(P4)</a>Difference method for parabolic equations  </li> 
    <li><a href="math/III/14.pdf#page=21">(P21)</a>Difference method for hyperbolic equations </li> 
  </ul>
</p></li>

<li><p>Character 15 : <strong>Finite Element Method</strong> <a href="math/III/15.pdf">[note]</a>
</p></li>
</ol>


<h3 id="partm4">Part IV : Optimization Algorithms</h3>
<ol>
<li><p>Character 1 : <strong>Convex Set & Convex Function & Convex Optimization</strong> <a href="math/IV/1.pdf">[note1]</a><a href="math/IV/2.pdf">[note2]</a><a href="math/IV/3.pdf">[note3]</a>
</p></li>
<li><p>Character 2 : <strong>LP & SDP & SOCP</strong> <a href="math/IV/4.pdf">[note1]</a><a href="math/IV/5.pdf">[note2]</a>
</p></li>
<li><p>Character 3 : <strong>Optimality theory</strong> <a href="math/IV/6.pdf">[note1]</a><a href="math/IV/7.pdf">[note2]</a>
</p></li>
<li><p>Character 4 : <strong>Optimality method</strong> 
  <ul> 
    <li><a href="math/IV/8.pdf">[note]</a>GD method</li> 
    <li><a href="math/IV/9.pdf">[note]</a>SubGD method</li> 
    <li><a href="math/IV/10.pdf">[note]</a>New method & Trust domains method</li>
    <li><a href="math/IV/11.pdf">[note]</a>Quasi-Newton's method(& Lasso)</li>
    <li><a href="math/IV/12.pdf">[note1]</a><a href="math/IV/13.pdf">[note2]</a>Approximate point gradient method</li>
    <li><a href="math/IV/14.pdf">[note]</a>Nesterov method</li> 
    <li><a href="math/IV/15.pdf">[note]</a>Augmentation of the Lagrangian method</li> 
    <li><a href="math/IV/16.pdf">[note]</a>Alternating direction multiplier method</li>
    <li><a href="math/IV/18.pdf">[note]</a>Duality algorithms</li>
    <li><a href="math/IV/19.pdf">[note]</a>Block coordinate descent method</li>
    <li><a href="math/IV/20.pdf">[note]</a>Semi-smooth Newton method</li>
  </ul>
</p></li>
<li><p>Character 5 : <strong>Simplex Method & Interior Point Method</strong> <a href="math/IV/l1.pdf">[note]</a>
</p></li>
<li><p>Character 6 : <strong>Compressed Sensing & Sparse Recovery Guarantees</strong> <a href="math/IV/l2.pdf">[note]</a>
</p></li>
<li><p>Character 7 : <strong>Compressed Sensing & Sparse Recovery Guarantees</strong> <a href="math/IV/l2.pdf">[note1]</a><a href="math/IV/l3.pdf">[note2]</a>
</p></li>
<li><p>Character 8 : <strong>Matrix Completion</strong> <a href="math/IV/l4.pdf">[note]</a>
</p></li>
<li><p>Character 9 : <strong>Integer Programming</strong> <a href="math/IV/l5.pdf">[note]</a>
</p></li>
<li><p>Character 10 : <strong>Submodular Function Optimization</strong> <a href="math/IV/l6.pdf">[note]</a>
</p></li>
<li><p>Character 11 : <strong>SG Method for Large-scales ML</strong> <a href="math/IV/l7.pdf">[note]</a>
</p></li>
<li><p>Character 12 : <strong>Randomized Numerical Linear Algebra</strong> <a href="math/IV/l8.pdf">[note]</a>
</p></li>
<li><p>Character 13 : <strong>Phase Retrieval</strong> <a href="math/IV/l9.pdf">[note]</a>
</p></li>
<li><p>Character 14 : <strong>Marcov Decision Process</strong> <a href="math/IV/l10.pdf">[note]</a>
</p></li>
<li><p>Character 15 : <strong>TD-learning and Q-learning</strong> <a href="math/IV/l11.pdf">[note]</a>
</p></li>
<li><p>Character 16 : <strong>Policy Gradient Methods</strong> <a href="math/IV/l12.pdf">[note]</a>
</p></li>
</ol>


<h2>What I have learned in cs</h2>

<h3 id="partc1">Part I : Data structures and algorithms</h3>
<ol>
<li><p>character 1 : Basic C++ Knowledge<a href="cs/I/char1.md">[note]</a>
</p></li>

</ol>

<h3 id="partc2">Part II : Parallel and Distributed Computing</h3>
<ol>
<li><p>Character 1 : <strong>Basic Theory</strong> <a href="cs/II/character1.pdf">[note]</a>
  <ul> 
    <li><a href="cs/II/character1.pdf#page=4">(P4)</a>Introduction</li> 
    <li><a href="cs/II/character1.pdf#page=43">(P43)</a>Hardware architecture</li> 
    <li><a href="cs/II/character1.pdf#page=61">(P61)</a>Parallel algorithms & Programming</li> 
    <li><a href="cs/II/character1.pdf#page=70">(P70)</a>Three laws of parallel computing</li>
    <li><a href="cs/II/character1.pdf#page=83">(P83)</a>TParallel computing models</li>
  </ul>
</p></li>


<li><p>Character 1.2 : <strong>Basic Theory(supplement)</strong> <a href="cs/II/character1.2.pdf">[note]</a>
</p></li>

<li><p>Character 2 : <strong>Programming & Practice: MPI</strong> <a href="cs/II/character2.pdf">[note]</a>
</p></li>

<li><p>Character 2 : <strong>Programming & Practice: OpenMP</strong> <a href="cs/II/character3.pdf">[note]</a>
</p></li>

<li><p>Character 2 : <strong>Programming & Practice: CUDA</strong> <a href="cs/II/character4.pdf">[note]</a>
</p></li>
<ol>


</td>
</tr>
</table>
</body>
</html>